<!DOCTYPE HTML>
<html lang="en">

<head>
  <title>Chi-Chang Lee</title>

  <!-- <link rel="icon" href="assests/icon.png" /> -->

  <link rel="stylesheet" type="text/css" href="assests/style.css">
  <link rel="stylesheet" type="text/css" href="assests/academicons.min.css">
  <script src="assests/fontawesome.js"></script>

  <script>
    function pressBtn(p) {

      var id_btn=["exp-btn", "pub-btn", "apub-btn"], class_btn=["fas fa-plus-square", "fas fa-minus-square", "fas fa-minus-square"];
      var id_div=["exp-div", "pub-div", "apub-div"], style_div=["height: 300px; overflow: auto;", "height: 400px; overflow: auto;", "height: 400px; overflow: auto;"];

      var btn = document.getElementById(id_btn[p]);
      if(btn.className == class_btn[0]) {
        btn.className = class_btn[1];
        document.getElementById(id_div[p]).style = "";
      }
      else {
        btn.className = class_btn[0];
        document.getElementById(id_div[p]).style = style_div[p];
      }
    }

    function img_hover(e, f) {
      e.setAttribute("src", f);
    }
    function img_unhover(e, f) {
      e.setAttribute("src", f);
    }
  </script>
</head>

<body>
  <table style="max-width: 900px;"><tbody>
    <tr style="padding: 0px;">
      <td style="padding: 0px;">

        <!--bio-->
        <table><tbody>
          <tr style="padding: 0px;">
            <td style="padding: 2%; width: 70%; vertical-align: middle;">

              <p style="text-align: center;">
                <name>Chi-Chang Lee</name>
              </p>

              <p>
                I am currently a research assistant in the <a href="https://sites.google.com/site/yitingchen0524/hcis-lab">HCIS Lab</a> at <a href="https://www.nycu.edu.tw/nycu/en/index">Yang Ming Chiao Tung University</a> in Taiwan, 
                working under the supervision of Prof. <a href="https://sites.google.com/site/yitingchen0524/home">Yi-Ting Chen</a>. 
                Since July 2023, I have also been collaborating with the <a herf="https://cap.csail.mit.edu/improbable-ai-lab-lab-tour">Improbable AI Lab</a>, 
                led by Prof. <a href="http://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a> at <a href="https://www.mit.edu/">Massachusetts Institute of Technology (MIT)</a>. 
                I earned both my Bachelor’s and Master’s degrees in Engineering Science and Computer Science from National Taiwan University, 
                where I worked closely with Prof. <a href="https://scholar.google.com/citations?hl=en&user=ZO5e5I4AAAAJ">Yu Tsao</a> and Prof. <a href="https://homepage.iis.sinica.edu.tw/pages/whm/index_en.html">Hsin-Min Wang</a>.
                Before 2023, my research primarily focused on task-aware front-end processing for downstream audio applications, with a particular emphasis on developing robust speech-to-text systems 
                (References: <a href="https://openreview.net/forum?id=5fvXH49wk2">ICLR'23</a> and <a href="https://arxiv.org/pdf/2311.16604">ASRU'23</a>). 
                
                <br><br>
                Beyond the input modality for models, I developed a keen interest in understanding how high-dimensional information from the perception stage influences downstream decision-making, and how these two modules can collaborate effectively. I believe that physical interaction with AI will be a pivotal area of focus in the next phase of AI development. Consequently, in 2023, I shifted my research focus to sensorimotor learning.
              <p style="text-align: center;">
                <a href="mailto:changlee@ntu.edu.tw" target="_blank"><i class="fas fa-envelope ai-2x"></i></a>&nbsp;
                <a href="https://github.com/ChangLee0903" target="_blank"><i class="fab fa-github ai-2x"></i></a>&nbsp;
                <a href="files/ccl-cv.pdf" target="_blank"><i class="ai ai-cv-square ai-2x"></i></a>&nbsp;
                <a href="https://scholar.google.com/citations?user=r225tRsAAAAJ&hl" target="_blank"><i class="ai ai-google-scholar-square ai-2x"></i></a>&nbsp;
              </p>
            </td>

            <td style="padding: 2%; width: 30%; max-width: 30%; top;">
              <img src="assests/ccl.jpg" style="border-radius: 100%;" onmouseover="img_hover(this, 'assests/ccl.jpg');" onmouseout="img_unhover(this, 'assests/ccl.jpg');" />
            </td>
          </tr>
        </tbody></table>
        <table><tbody>
          <tr>
            <td class="all">
              <heading>
                <i class="fas fa-briefcase"></i>&nbsp;&nbsp;Research<i class="fas fa-plus-square" style="float: right;" onclick="pressBtn(0)" id="exp-btn"></i>
              </heading>
            </td>
          </tr>
        </tbody></table>
        <div style="height: 300px; overflow: auto;" id="exp-div">
          <p style="text-align: left;">    
            My current works now center on the following two areas:
                <ul>
                  <li><b>Harnessing Heuristic and its Robustness for DRL:</b> 
                    In practice, DRL training frequently relies on heuristic rewards to promote survival and exploration in complex environments, 
                    primarily due to the limitation of finite sample sizes for each training batch. 
                    These finite sample conditions hinder the empirical feasibility of achieving the ideal expectations typically required by theoretical standards, 
                    such as potential-based reward shaping. 
                    My recent work addresses the exploitation-exploration dilemma under finite sample conditions, 
                    enabling more feasible approximations of optimal policies while improving the robustness of heuristic selections with significantly reduced design effort. (References: <a href="https://changlee0903.github.io/pubs/HEPO.pdf">NeurIPS'24</a> and <a href="https://srinathm1359.github.io/data/Maximizing_Quadruped_Velocity_by_Minimizing_Energy.pdf">ICRA'24</a>)
                  <li><b>Connecting Entanglement in Perceivability and Decision-Making:</b> 
                    An agent cannot effectively make decisions without perceiving the necessary patterns. While trajectories for policy training are collected by experts using privileged information, but only sensor features (partial observations) are available during deployment, reproducing the same decision-making outcomes becomes challenging due to insufficient patterns in these partial observations. Even if a perception model might converge to a local minimum with similar total loss levels, different error situations can indicate varying degrees of perceivability, significantly impacting decision-making. My earlier work focused on optimizing perception model training to ensure it aligns with both perception and downstream objectives. In addition, I am currently working on a project at the <a href="https://sites.google.com/site/yitingchen0524/hcis-lab">HCIS Lab</a> that investigates how perceivability can be used to weigh action likelihoods in policy learning while minimizing the precision requirements of perception for decision-making.
                    (References: <a href="https://arxiv.org/pdf/2402.19464.pdf">ICLR'24</a>)                    
                  </li> 
                </ul>
              </p>
        </div>
        <br><br>
        <!--experience-->
        <table><tbody>
          <tr>
            <td class="all">
              <heading>
                <i class="fas fa-briefcase"></i>&nbsp;&nbsp;Experience<i class="fas fa-plus-square" style="float: right;" onclick="pressBtn(0)" id="exp-btn"></i>
              </heading>
            </td>
          </tr>
        </tbody></table>
        <br>
        <div style="height: 300px; overflow: auto;" id="exp-div">
          <table><tbody>
            <tr>
              <td class="exp-avatar">
                <img src="imgs/avatar_mit.png" />
              </td>

              <td class="exp-description">
                <b>Research Collaborator</b>
                <span style="float: right;">Jul. 2023 – present</span>
                <br>
                <a href="https://cap.csail.mit.edu/improbable-ai-lab-lab-tour" target="_blank">Improbable AI Lab at Massachusetts Institute of Technology</a>
                <br>
                Advisor: <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>
              </td>
            </tr>

            <tr>
              <td class="exp-avatar">
                <img src="imgs/hcis.png" />
              </td>

              <td class="exp-description">
                <b>Research Assistant</b>
                <span style="float: right;">Mar. 2024 – present</span>
                <br>
                <a href="https://sites.google.com/site/yitingchen0524/hcis-lab" target="_blank">HCIS Lab at National Chiao Tung University, Taiwan</a>
                <br>
                Advisor: <a href="https://sites.google.com/site/yitingchen0524/home">Yi-Ting Chen</a>
              </td>
            </tr>
            <tr>
                <td class="exp-avatar">
                  <img src="imgs/CITI.jpeg" />
                </td>

                <td class="exp-description">
                  <b>Research Assistant</b>
                  <span style="float: right;">Mar. 2019 – Mar. 2024</span>
                  <br>
                  <a href="https://bio-asplab.citi.sinica.edu.tw" target="_blank">Bio-ASP Lab at Academia Sinica CITI, Taiwan</a>
                  <br>
                  Advisor: <a href="https://scholar.google.com/citations?hl=en&user=ZO5e5I4AAAAJ">Yu Tsao</a>
                </td>
              </tr>
              <tr>
                <td class="exp-avatar">
                  <img src="imgs/NII.jpeg" />
                </td>

                <td class="exp-description">
                  <b>Visiting Researcher</b>
                  <span style="float: right;">Nov. 2022 – Feb. 2023</span>
                  <br>
                  <a href="https://nii-yamagishilab.github.io" target="_blank">Yamagishi Laboratory at National Institute of Informatics, Japan</a>
                  <br>
                  Advisor: Prof. <a href="https://researchmap.jp/read0205283" target="_blank">Junichi Yamagishi</a>
                </td>
              </tr>
          </tbody></table>
        </div>
        <br><br>
        
        <!--publications-->
        <table><tbody>
          <tr>
            <td class="all">
              <heading>
                <i class="far fa-file-alt"></i>&nbsp;&nbsp;Selected Publications in Sensorimotor Learning<i class="fas fa-plus-square" style="float: right;" onclick="pressBtn(1)" id="pub-btn"></i>
              </heading>
              <br>(∗ indicates equal contribution)
            </td>
          </tr>
        </tbody></table>
        <br>
        <div style="height: 400px; overflow: auto;" id="pub-div">
          <table><tbody>
            <tr>
              <td class="pub-avatar">
                <img src="imgs/HEPO.gif" />
              </td>
              <td class="pub-description">
                <papertitle>Harnessing Heuristics for Deep Reinforcement Learning via Constrained Optimization</papertitle>
                <br>
                <I><u><b>Chi-Chang Lee*</b></u>, Zhang-Wei Hong*, Pulkit Agrawal</I>             
                <br>
                Submitted to Conference on Neural Information Processing Systems (NeurIPS) 2024, currently scored as ‘Weak Accept’, ‘Accept’, and ‘Accept’
                <br>
                  <a href="https://changlee0903.github.io/pubs/HEPO.pdf" target="_blank">Paper (submitting)</a>
              </td>
            </tr>
          </tbody></table>
          <br>
          <table><tbody>
            <tr>
              <td class="pub-avatar">
                <img src="imgs/avatar_eipo_icra.gif" />
              </td>
              <td class="pub-description">
                <papertitle>Maximizing Velocity by Minimizing Energy</papertitle>
                <br>
                <I>Srinath Mahankali*, <u><b>Chi-Chang Lee*</b></u>, Gabriel B. Margolis, Zhang-Wei Hong, Pulkit Agrawal</I>             
                <br>
                  International Conference on Robotics and Automation (ICRA), 2024
                <br>
                  <a href="https://srinathm1359.github.io/data/Maximizing_Quadruped_Velocity_by_Minimizing_Energy.pdf" target="_blank">Paper</a> |
                  <a href="https://srinathm1359.github.io/eipo-locomotion" target="_blank">Website</a> |
                  <a href="https://github.com/Improbable-AI/walk-these-ways" target="_blank">Code</a>
              </td>
            </tr>
          </tbody></table>
          <br>
          <table><tbody>
            <tr>
              <td class="pub-avatar">
                <img src="imgs/D4AM.png" />
              </td>
              <td class="pub-description">
                <papertitle>D4AM: A General Denoising Framework for Downstream Acoustic Models</papertitle>
                <br>
                <I><u><b>Chi-Chang Lee</b></u>, Yu Tsao, Hsin-Min Wang, Chu-Song Chen</I>                     
                <br>
                  International Conference on Learning Representations (ICLR), 2023
                <br>
                  <a href="https://openreview.net/forum?id=5fvXH49wk2" target="_blank">Paper</a> |
                  <a href="https://github.com/ChangLee0903/D4AM" target="_blank">Code</a>
              </td>
            </tr>
          </tbody></table>
          </div>
        <br><br>
        <!--publications-->
        <table><tbody>
          <tr>
            <td class="all">
              <heading>
                <i class="far fa-file-alt"></i>&nbsp;&nbsp;Publications in Audio Applications<i class="fas fa-plus-square" style="float: right;" onclick="pressBtn(2)" id="apub-btn"></i>
              </heading>
            </td>
          </tr>
        </tbody></table>
        <br>
        <div style="height: 400px; overflow: auto;" id="apub-div">
          <table><tbody>
            <tr>
              <td class="pub-avatar">
                <img src="imgs/lc4sv.png" />
              </td>
              <td class="pub-description">
                <papertitle>LC4SV: A Denoising Framework Learning to Compensate for Unseen Speaker Verification Models</papertitle>
                <br>
                <I><u><b>Chi-Chang Lee</b></u>, Hong-Wei Chen, Chu-Song Chen, Hsin-Min Wang, Tsung-Te Liu, Yu Tsao</I>             
                <br>
                IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2023
                <br>
                <a href="https://arxiv.org/pdf/2311.16604" target="_blank">Paper</a> |
                  <a href="https://github.com/ChangLee0903/D4AM" target="_blank">Code</a>
              </td>
            </tr>
          </tbody></table>
          <br>
          <table><tbody>
            <tr>
              <td class="pub-avatar">
                <img src="imgs/nastar.png"/>
              </td>
              <td class="pub-description">
                <papertitle>NASTAR: Noise Adaptive Speech Enhancement with Target-Conditional Resampling</papertitle>
                <br>
                <I><u><b>Chi-Chang Lee</b></u>, Cheng-Hung Hu, Yu-Chen Lin, Chu-Song Chen, Hsin-Min Wang, Yu Tsao</I>             
                <br>
                  INTERSPEECH, 2022
                <br>
                  <a href="https://arxiv.org/pdf/2206.09058" target="_blank">Paper</a> |
                  <a href="https://changlee0903.github.io/NASTAR-demo/" target="_blank">Website</a> |
                  <a href="https://github.com/ChangLee0903/NASTAR" target="_blank">Code</a>
              </td>
            </tr>
          </tbody></table>
          <br>
          <table><tbody>
            <tr>
              <td class="pub-avatar">
                <img src="imgs/seril.jpg" />
              </td>
              <td class="pub-description">
                <papertitle>SERIL: Noise adaptive speech enhancement using regularization-based incremental learning</papertitle>
                <br>
                <I><u><b>Chi-Chang Lee</b></u>, Yu-Chen Lin, Hsuan-Tien Lin, Hsin-Min Wang, Yu Tsao</I>                     
                <br>
                INTERSPEECH, 2020
                <br>
                  <a href="https://arxiv.org/pdf/2005.11760" target="_blank">Paper</a> |
                  <a href=" https://github.com/ChangLee0903/SERIL" target="_blank">Code</a>
              </td>
            </tr>
          </tbody></table>
          </div>
        <br><br>

        <!--teaching-->
        <table><tbody>
            <tr>
              <td class="all">
                <heading><i class="fas fa-chalkboard-teacher"></i>&nbsp;&nbsp;Teaching</heading>
              </td>
            </tr>
            <tr>
              <td class="all">

                <br>
                <b>Teaching Assistant</b>, <a href="https://ntueeml.github.io/ml-website/" target="_blank">Machine Learning</a>, National Taiwan University, Taiwan
                <span style="float: right">2021 Fall</span>

              </td>
            </tr>

            <tr>
              <td class="all">

                <br>
                <b>Teaching Assistant</b>, <a href="https://djj.ee.ntu.edu.tw/TFW.htm" target="_blank">Time Frequency Analysis and Wavelet Transforms</a>, National Taiwan University, Taiwan
                <span style="float: right">2018 Fall</span>

              </td>
            </tr>

        </tbody></table>
        <br><br>

        <!--honors-->
        <table><tbody>
          <tr>
            <td class="all">
              <heading><i class="fas fa-chalkboard-teacher"></i>&nbsp;&nbsp;Honor</heading>
            </td>
          </tr>
          <tr>
            <td class="all">

              <br>
              <b>Second Place</b>, <a href="https://www.iccad-contest.org/2019/" target="_blank">IC/CAD Contest 2019</a>

            </td>
          </tr>

      </tbody></table>
      <br><br>

        <!--boreder-->
        <table><tbody>
          <tr>
            <td style="padding: 0px">
              <br>
              <p style="text-align: center;">
                template from <a href="https://jonbarron.info" target="_blank">jonbarron</a>
              </p>
            </td>
          </tr>
        </tbody></table>

      </td>
    </tr>
  </table>
</body>

</html>
